{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Brief\n",
    "A tutorial for CBOW model within TensorFlow (API r1.3) framework. \n",
    "## Reference\n",
    "[Lecture note from Stanford](http://cs224d.stanford.edu/lecture_notes/notes1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_path = os.path.normpath(\"../Dataset/arvix_abstracts.txt\")\n",
    "with open(corpus_path,\"r\") as f:\n",
    "    corpus = \"\".join(f.readlines()).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define class for codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WordCodec:\n",
    "    def __init__(self, word_flow):\n",
    "        self._index_to_word = []\n",
    "        self._word_to_index = {}\n",
    "        for word in word_flow:\n",
    "            assert type(word)!=\"str\", \"Got type {} instead of str\".format(type(word))\n",
    "            if word not in self._word_to_index:\n",
    "                self._word_to_index[word] = len(self._index_to_word)\n",
    "                self._index_to_word.append(word)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == int:\n",
    "            return self._index_to_word[key]\n",
    "        elif type(key) == str:\n",
    "            return self._word_to_index[key]\n",
    "        else:\n",
    "            raise TypeError(\"key must be either int or str.\")\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_flow():\n",
    "    for paragraph in corpus:\n",
    "        for word in paragraph.split(\" \"):\n",
    "            yield word\n",
    "one_hot_codec = WordCodec(word_flow())\n",
    "print(\"Total number of word in the vocabulary: {}\".format(one_hot_codec.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, context_length, embedding_dim):\n",
    "        with tf.variable_scope(\"CBOW\"):\n",
    "            self._context_input = tf.placeholder(shape=[2*context_length], dtype=tf.int32)\n",
    "            V = tf.get_variable(shape=[vocab_size, embedding_dim], dtype=tf.float32, name=\"V\")  # Embedding\n",
    "            self._embedding = tf.nn.embedding_lookup(V, self._context_input)\n",
    "            hidden = tf.reduce_mean(self._embedding, axis=0, keep_dims=True)\n",
    "            U = tf.get_variable(shape=[embedding_dim, vocab_size], name=\"U\")\n",
    "            self._output = tf.matmul(hidden, U)\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._context_input\n",
    "    \n",
    "    @property\n",
    "    def output(self):\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_length = 2\n",
    "embedding_dim = 100\n",
    "cbow_model = CBOW(one_hot_codec.vocab_size, context_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Metadata File for Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_path = os.path.normpath(\"./graphs/word_codec\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    f.write(\"Index\\tWord\\n\")\n",
    "    for i in range(one_hot_codec.vocab_size):\n",
    "        f.write(\"{}\\t{}\\n\".format(i, one_hot_codec[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_sample_generator(corpus, codec, context_length):\n",
    "    for paragraph in corpus:\n",
    "        paragraph = np.array([codec[word] for word in paragraph.split(\" \")])\n",
    "        for i in range(context_length, np.shape(paragraph)[0]-context_length):\n",
    "            yield np.concatenate([paragraph[i-context_length:i], paragraph[i+1:i+context_length+1]],axis=0), paragraph[i:i+1]\n",
    "target_output=tf.placeholder(shape=[1],dtype=tf.int32, name=\"target_output\")\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cbow_model.output, labels=target_output))\n",
    "lr = 1e-4\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)\n",
    "training_set = training_sample_generator(corpus, one_hot_codec, context_length)\n",
    "with tf.name_scope(\"summary\") as scope:\n",
    "    summary_op = tf.summary.scalar(name=\"loss\",tensor=loss)\n",
    "num_epoch = 20\n",
    "graph_path = \"./graphs\"\n",
    "model_checkpoint_path = os.path.join(\"./graphs\", \"CBOW\")\n",
    "save_every = 1000\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(logdir=graph_path, graph=sess.graph)\n",
    "    ckpt = tf.train.get_checkpoint_state(model_checkpoint_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Continue training\")\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epoch):\n",
    "        for x, y in training_set:\n",
    "            _, summary = sess.run([train_op, summary_op], feed_dict={cbow_model.input: x, target_output: y})\n",
    "            writer.add_summary(summary=summary, global_step=global_step.eval(sess))\n",
    "            n_iter = global_step.eval(sess)\n",
    "            if (n_iter % save_every) == 0:\n",
    "                saver.save(sess=sess, save_path=model_checkpoint_path, global_step=n_iter)   \n",
    "        saver.save(sess=sess, save_path=model_checkpoint_path, global_step=n_iter)\n",
    "    print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
